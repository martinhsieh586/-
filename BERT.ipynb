{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä¸‹è¼‰ dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\class\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "d:\\anaconda3\\envs\\class\\lib\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Fetching 4 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'datasets--takala--financial_phrasebank\\\\snapshots\\\\1484d06fe7af23030c7c977b12556108d1f67039'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from huggingface_hub import snapshot_download\n",
    " \n",
    "# ä¸‹è¼‰è³‡æ–™é›†\n",
    "snapshot_download(repo_id=\"takala/financial_phrasebank\", repo_type=\"dataset\",\n",
    "                  cache_dir=\"\",\n",
    "                  local_dir_use_symlinks=False, resume_download=True,\n",
    "                  token='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')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1811/1811 [00:00<00:00, 2545.52 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 453/453 [00:00<00:00, 2624.95 examples/s]\n",
      "d:\\anaconda3\\envs\\class\\lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\è¬å­Ÿé”\\AppData\\Local\\Temp\\ipykernel_7740\\1786074241.py:66: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "  3%|â–Ž         | 10/342 [00:07<02:23,  2.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9191, 'grad_norm': 5.645734786987305, 'learning_rate': 4.868421052631579e-05, 'epoch': 0.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|â–Œ         | 20/342 [00:13<02:14,  2.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6572, 'grad_norm': 6.836689472198486, 'learning_rate': 4.722222222222222e-05, 'epoch': 0.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|â–‰         | 30/342 [00:19<02:09,  2.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5327, 'grad_norm': 6.590019702911377, 'learning_rate': 4.576023391812866e-05, 'epoch': 0.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|â–ˆâ–        | 40/342 [00:25<02:05,  2.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5532, 'grad_norm': 9.049614906311035, 'learning_rate': 4.429824561403509e-05, 'epoch': 0.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|â–ˆâ–        | 50/342 [00:31<02:01,  2.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3361, 'grad_norm': 5.400299549102783, 'learning_rate': 4.283625730994152e-05, 'epoch': 0.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|â–ˆâ–Š        | 60/342 [00:37<01:58,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2801, 'grad_norm': 7.022729873657227, 'learning_rate': 4.137426900584795e-05, 'epoch': 0.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|â–ˆâ–ˆ        | 70/342 [00:45<02:00,  2.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.21, 'grad_norm': 3.3065104484558105, 'learning_rate': 3.991228070175439e-05, 'epoch': 0.61}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|â–ˆâ–ˆâ–Ž       | 80/342 [00:51<01:49,  2.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1157, 'grad_norm': 6.795736789703369, 'learning_rate': 3.845029239766082e-05, 'epoch': 0.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|â–ˆâ–ˆâ–‹       | 90/342 [00:57<01:45,  2.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1533, 'grad_norm': 4.032856464385986, 'learning_rate': 3.6988304093567254e-05, 'epoch': 0.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|â–ˆâ–ˆâ–‰       | 100/342 [01:05<01:45,  2.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1464, 'grad_norm': 6.182636737823486, 'learning_rate': 3.5526315789473684e-05, 'epoch': 0.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|â–ˆâ–ˆâ–ˆâ–      | 110/342 [01:12<01:41,  2.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2537, 'grad_norm': 0.42610445618629456, 'learning_rate': 3.406432748538012e-05, 'epoch': 0.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 114/342 [01:18<02:02,  1.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1049937978386879, 'eval_runtime': 3.1675, 'eval_samples_per_second': 143.016, 'eval_steps_per_second': 9.156, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 120/342 [01:21<02:06,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1076, 'grad_norm': 0.323152631521225, 'learning_rate': 3.274853801169591e-05, 'epoch': 1.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|â–ˆâ–ˆâ–ˆâ–Š      | 130/342 [01:27<01:30,  2.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1228, 'grad_norm': 12.573695182800293, 'learning_rate': 3.128654970760234e-05, 'epoch': 1.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 140/342 [01:35<01:30,  2.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0342, 'grad_norm': 0.12160547077655792, 'learning_rate': 2.9824561403508772e-05, 'epoch': 1.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 150/342 [01:42<01:23,  2.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1393, 'grad_norm': 0.16723516583442688, 'learning_rate': 2.8362573099415208e-05, 'epoch': 1.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 160/342 [01:48<01:16,  2.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0409, 'grad_norm': 0.20046114921569824, 'learning_rate': 2.6900584795321637e-05, 'epoch': 1.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 170/342 [01:54<01:12,  2.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1311, 'grad_norm': 0.11337663978338242, 'learning_rate': 2.5438596491228074e-05, 'epoch': 1.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 180/342 [02:01<01:10,  2.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1002, 'grad_norm': 5.323081970214844, 'learning_rate': 2.3976608187134503e-05, 'epoch': 1.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 190/342 [02:08<01:07,  2.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1361, 'grad_norm': 0.0889088436961174, 'learning_rate': 2.2514619883040936e-05, 'epoch': 1.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 200/342 [02:15<01:00,  2.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0598, 'grad_norm': 1.974332332611084, 'learning_rate': 2.105263157894737e-05, 'epoch': 1.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 210/342 [02:21<00:56,  2.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0997, 'grad_norm': 1.0070358514785767, 'learning_rate': 1.9590643274853802e-05, 'epoch': 1.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 220/342 [02:27<00:50,  2.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1034, 'grad_norm': 0.5035066604614258, 'learning_rate': 1.8128654970760235e-05, 'epoch': 1.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 228/342 [02:36<00:45,  2.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.11506521701812744, 'eval_runtime': 3.031, 'eval_samples_per_second': 149.456, 'eval_steps_per_second': 9.568, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 230/342 [02:37<01:54,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0419, 'grad_norm': 2.66349720954895, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 240/342 [02:43<00:43,  2.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0102, 'grad_norm': 0.0621904581785202, 'learning_rate': 1.5204678362573099e-05, 'epoch': 2.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 250/342 [02:49<00:37,  2.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0154, 'grad_norm': 1.4612834453582764, 'learning_rate': 1.3742690058479531e-05, 'epoch': 2.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 260/342 [02:56<00:33,  2.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0431, 'grad_norm': 0.03354426845908165, 'learning_rate': 1.2280701754385964e-05, 'epoch': 2.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 270/342 [03:04<00:30,  2.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0198, 'grad_norm': 0.03577283397316933, 'learning_rate': 1.0818713450292397e-05, 'epoch': 2.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 280/342 [03:09<00:24,  2.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0342, 'grad_norm': 0.4569975435733795, 'learning_rate': 9.35672514619883e-06, 'epoch': 2.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 290/342 [03:15<00:20,  2.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0812, 'grad_norm': 0.24573484063148499, 'learning_rate': 7.894736842105263e-06, 'epoch': 2.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 300/342 [03:27<00:19,  2.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.017, 'grad_norm': 0.07203696668148041, 'learning_rate': 6.432748538011696e-06, 'epoch': 2.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 310/342 [03:40<00:15,  2.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0716, 'grad_norm': 0.03813231736421585, 'learning_rate': 4.970760233918129e-06, 'epoch': 2.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 320/342 [03:46<00:08,  2.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0407, 'grad_norm': 0.18236462771892548, 'learning_rate': 3.5087719298245615e-06, 'epoch': 2.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 330/342 [04:03<00:06,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0177, 'grad_norm': 0.07016615569591522, 'learning_rate': 2.0467836257309943e-06, 'epoch': 2.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 340/342 [04:12<00:00,  2.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0271, 'grad_norm': 0.053797993808984756, 'learning_rate': 5.847953216374269e-07, 'epoch': 2.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 342/342 [04:21<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.12069191783666611, 'eval_runtime': 3.4001, 'eval_samples_per_second': 133.229, 'eval_steps_per_second': 8.529, 'epoch': 3.0}\n",
      "{'train_runtime': 261.1558, 'train_samples_per_second': 20.804, 'train_steps_per_second': 1.31, 'train_loss': 0.16529753169573147, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:03<00:00,  8.80it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.12069191783666611,\n",
       " 'eval_runtime': 3.3123,\n",
       " 'eval_samples_per_second': 136.764,\n",
       " 'eval_steps_per_second': 8.755,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# æª¢æŸ¥ GPU æ˜¯å¦å¯ç”¨\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# è®€å–æ•¸æ“š\n",
    "def load_data(file_path):\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    label_mapping = {\"positive\": 2, \"neutral\": 1, \"negative\": 0}\n",
    "    \n",
    "    with open(file_path, \"r\",encoding='latin1') as file:\n",
    "        for line in file:\n",
    "            # åˆ†å‰²å¥å­èˆ‡æƒ…æ„Ÿæ¨™ç±¤\n",
    "            sentence, sentiment = line.strip().rsplit(\"@\", 1)\n",
    "            sentences.append(sentence)\n",
    "            labels.append(label_mapping[sentiment])\n",
    "    \n",
    "    return pd.DataFrame({\"sentence\": sentences, \"label\": labels})\n",
    "\n",
    "# åŠ è¼‰ä¸¦è™•ç†è³‡æ–™\n",
    "file_path = \"./datasets--takala--financial_phrasebank/data/FinancialPhraseBank-v1.0/Sentences_AllAgree.txt\"  # æ›¿æ›ç‚ºæ­£ç¢ºçš„æª”æ¡ˆè·¯å¾‘\n",
    "data = load_data(file_path)\n",
    "\n",
    "# åˆ†å‰²è³‡æ–™é›†\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    data[\"sentence\"], data[\"label\"], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# è½‰æ›ç‚ºDatasetsæ ¼å¼\n",
    "train_dataset = Dataset.from_dict({\"text\": train_texts, \"label\": train_labels})\n",
    "val_dataset = Dataset.from_dict({\"text\": val_texts, \"label\": val_labels})\n",
    "\n",
    "# åˆå§‹åŒ– BERT\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=3)\n",
    "model.to(device)  # å°‡æ¨¡åž‹ç§»è‡³ GPU\n",
    "\n",
    "# Tokenization\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# è¨­ç½®æ ¼å¼ç‚º PyTorch tensors\n",
    "train_dataset = train_dataset.with_format(\"torch\")\n",
    "val_dataset = val_dataset.with_format(\"torch\")\n",
    "\n",
    "# è¨“ç·´åƒæ•¸è¨­ç½®\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_steps=10,\n",
    "    report_to=\"none\",  # ç¦ç”¨é»˜èªçš„å ±å‘Šå™¨ï¼ˆä¾‹å¦‚ wandbï¼‰\n",
    "    fp16=True  # å•Ÿç”¨ 16 ä½æµ®é»žæ•¸è¨ˆç®—ï¼ˆæ··åˆç²¾åº¦è¨“ç·´ï¼‰\n",
    ")\n",
    "\n",
    "# å®šç¾©è¨“ç·´å™¨\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# é–‹å§‹è¨“ç·´\n",
    "trainer.train()\n",
    "\n",
    "# æ¨¡åž‹è©•ä¼°\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Postive', 'Negative', 'Postive']\n"
     ]
    }
   ],
   "source": [
    "model\n",
    "test_texts = [\n",
    "\"The company's profit has increased significantly this quarter.\",\n",
    "\"The increase in costs negatively affected the revenue.\",\n",
    "\"The company's performance remianed stable.\"\n",
    "]\n",
    "\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True, return_tensors=\"pt\").to(device)\n",
    "\n",
    "outputs = model(**test_encodings)\n",
    "\n",
    "preds = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n",
    "\n",
    "label_map = {0:\"Negative\",1:\"Neutral\", 2:\"Postive\"}\n",
    "predicted_labels = [label_map[pred] for pred in preds]\n",
    "print(predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "class",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
