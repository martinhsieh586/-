{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockDataset(Dataset):\n",
    "    def __init__(self, image_dir, label_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.transform = transform\n",
    "        self.image_names = os.listdir(image_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_name = self.image_names[idx]\n",
    "        image_path = os.path.join(self.image_dir, image_name)\n",
    "        label_path = os.path.join(self.label_dir, image_name.replace(\".png\", \".txt\"))\n",
    "\n",
    "        # 讀取影像\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # 讀取標籤並轉換為 int 型態\n",
    "        with open(label_path, \"r\") as f:\n",
    "            label = int(f.read().strip())  # 確保標籤為 int 類型\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# 影像變換\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),  # 調整影像大小\n",
    "    transforms.ToTensor(),        # 轉換為 Tensor 格式\n",
    "])\n",
    "\n",
    "# 建立資料集與資料加載器\n",
    "train_dataset = StockDataset(image_dir=\"./train_data/image\", label_dir=\"./train_data/label\", transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=2)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        # 自適應池化層將輸出調整為固定大小，這裡設置為 3x3\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((3, 3))\n",
    "        \n",
    "        # 全連接層\n",
    "        self.fc1 = nn.Linear(128 * 3 * 3, 256)\n",
    "        self.fc2 = nn.Linear(256, 2)  # 最後輸出 1 個數字，不再使用 Sigmoid\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.adaptive_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x  # 直接返回線性輸出，不使用 Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500, Loss: 0.7005655765533447\n",
      "Epoch 2/500, Loss: 0.692652336188725\n",
      "Epoch 3/500, Loss: 0.6749545591218131\n",
      "Epoch 4/500, Loss: 0.6495191965784345\n",
      "Epoch 5/500, Loss: 0.6044888070651463\n",
      "Epoch 6/500, Loss: 0.5476020957742419\n",
      "Epoch 7/500, Loss: 0.4929038882255554\n",
      "Epoch 8/500, Loss: 0.43646055459976196\n",
      "Epoch 9/500, Loss: 0.4291723498276302\n",
      "Epoch 10/500, Loss: 0.38941396134240286\n",
      "Epoch 11/500, Loss: 0.3442677514893668\n",
      "Epoch 12/500, Loss: 0.33065531722136904\n",
      "Epoch 13/500, Loss: 0.28832243170057026\n",
      "Epoch 14/500, Loss: 0.2551618835755757\n",
      "Epoch 15/500, Loss: 0.23496026545763016\n",
      "Epoch 16/500, Loss: 0.2222244686314038\n",
      "Epoch 17/500, Loss: 0.2092806931052889\n",
      "Epoch 18/500, Loss: 0.17648611749921525\n",
      "Epoch 19/500, Loss: 0.1890683929835047\n",
      "Epoch 20/500, Loss: 0.16747991421392985\n",
      "Epoch 21/500, Loss: 0.14871336200407573\n",
      "Epoch 22/500, Loss: 0.13454450986215047\n",
      "Epoch 23/500, Loss: 0.13347047248056956\n",
      "Epoch 24/500, Loss: 0.11593397387436458\n",
      "Epoch 25/500, Loss: 0.10286049172282219\n",
      "Epoch 26/500, Loss: 0.11438789750848498\n",
      "Epoch 27/500, Loss: 0.09813431064997401\n",
      "Epoch 28/500, Loss: 0.09134673194161483\n",
      "Epoch 29/500, Loss: 0.07719429582357407\n",
      "Epoch 30/500, Loss: 0.07295864660825048\n",
      "Epoch 31/500, Loss: 0.06647646240890026\n",
      "Epoch 32/500, Loss: 0.06527754212064403\n",
      "Epoch 33/500, Loss: 0.05664499689425741\n",
      "Epoch 34/500, Loss: 0.040520987074290006\n",
      "Epoch 35/500, Loss: 0.04176052659749985\n",
      "Epoch 36/500, Loss: 0.03608913280602012\n",
      "Epoch 37/500, Loss: 0.038761354716760774\n",
      "Epoch 38/500, Loss: 0.027173563039728572\n",
      "Epoch 39/500, Loss: 0.025532620027661324\n",
      "Epoch 40/500, Loss: 0.02191890257277659\n",
      "Epoch 41/500, Loss: 0.018434293568134308\n",
      "Epoch 42/500, Loss: 0.021667889718498503\n",
      "Epoch 43/500, Loss: 0.019308843383831636\n",
      "Epoch 44/500, Loss: 0.015864883749080554\n",
      "Epoch 45/500, Loss: 0.01245885984306889\n",
      "Epoch 46/500, Loss: 0.012312508254711117\n",
      "Epoch 47/500, Loss: 0.00866672902234963\n",
      "Epoch 48/500, Loss: 0.008483503546033586\n",
      "Epoch 49/500, Loss: 0.006444993295839855\n",
      "Epoch 50/500, Loss: 0.006930771217282329\n",
      "Epoch 51/500, Loss: 0.007225025478484375\n",
      "Epoch 52/500, Loss: 0.006968624590496931\n",
      "Epoch 53/500, Loss: 0.008267364464700222\n",
      "Epoch 54/500, Loss: 0.00851963014741029\n",
      "Epoch 55/500, Loss: 0.00758455487500344\n",
      "Epoch 56/500, Loss: 0.004209881515375206\n",
      "Epoch 57/500, Loss: 0.004428628200132932\n",
      "Epoch 58/500, Loss: 0.003867024605694626\n",
      "Epoch 59/500, Loss: 0.003497808988738273\n",
      "Epoch 60/500, Loss: 0.003435951840531613\n",
      "Epoch 61/500, Loss: 0.0029737279733775984\n",
      "Epoch 62/500, Loss: 0.0029187040761046645\n",
      "Epoch 63/500, Loss: 0.002469918574206531\n",
      "Epoch 64/500, Loss: 0.002692137695183711\n",
      "Epoch 65/500, Loss: 0.0025224388061490443\n",
      "Epoch 66/500, Loss: 0.0026905598185424295\n",
      "Epoch 67/500, Loss: 0.0019425669618483102\n",
      "Epoch 68/500, Loss: 0.001954673182418836\n",
      "Epoch 69/500, Loss: 0.001908548766680594\n",
      "Epoch 70/500, Loss: 0.0018575578911363014\n",
      "Epoch 71/500, Loss: 0.0016153614768492325\n",
      "Epoch 72/500, Loss: 0.0019911738678014706\n",
      "Epoch 73/500, Loss: 0.001349555278596069\n",
      "Epoch 74/500, Loss: 0.001393288403050974\n",
      "Epoch 75/500, Loss: 0.0021786054795874016\n",
      "Epoch 76/500, Loss: 0.001774782601777198\n",
      "Epoch 77/500, Loss: 0.001552273444498756\n",
      "Epoch 78/500, Loss: 0.0014688704312512918\n",
      "Epoch 79/500, Loss: 0.0012546318799390324\n",
      "Epoch 80/500, Loss: 0.001394771322208856\n",
      "Epoch 81/500, Loss: 0.0013069999944751284\n",
      "Epoch 82/500, Loss: 0.0012565472861751914\n",
      "Epoch 83/500, Loss: 0.0011814905613261675\n",
      "Epoch 84/500, Loss: 0.0011927343771925994\n",
      "Epoch 85/500, Loss: 0.0012793780437537602\n",
      "Epoch 86/500, Loss: 0.0016146884632429906\n",
      "Epoch 87/500, Loss: 0.0008650695755412536\n",
      "Epoch 88/500, Loss: 0.0009904206755371498\n",
      "Epoch 89/500, Loss: 0.0009270549365153004\n",
      "Epoch 90/500, Loss: 0.0008059325378521212\n",
      "Epoch 91/500, Loss: 0.0010031266470572778\n",
      "Epoch 92/500, Loss: 0.0008728994164682392\n",
      "Epoch 93/500, Loss: 0.0008373666787520051\n",
      "Epoch 94/500, Loss: 0.0009144955464372677\n",
      "Epoch 95/500, Loss: 0.0008053458462070141\n",
      "Epoch 96/500, Loss: 0.0007177854755095073\n",
      "Epoch 97/500, Loss: 0.0007086253442269351\n",
      "Epoch 98/500, Loss: 0.0008220791143165636\n",
      "Epoch 99/500, Loss: 0.000795761395628298\n",
      "Epoch 100/500, Loss: 0.00046966170027319876\n",
      "Epoch 101/500, Loss: 0.0006171903764230333\n",
      "Epoch 102/500, Loss: 0.0005827367775574592\n",
      "Epoch 103/500, Loss: 0.0007797012845652976\n",
      "Epoch 104/500, Loss: 0.0006479610871922757\n",
      "Epoch 105/500, Loss: 0.000475850939567733\n",
      "Epoch 106/500, Loss: 0.0005251403740008495\n",
      "Epoch 107/500, Loss: 0.0006633336826260867\n",
      "Epoch 108/500, Loss: 0.0006623468486525651\n",
      "Epoch 109/500, Loss: 0.0005293374852044508\n",
      "Epoch 110/500, Loss: 0.0008138735678845219\n",
      "Epoch 111/500, Loss: 0.0005253517426483865\n",
      "Epoch 112/500, Loss: 0.0005247971130302176\n",
      "Epoch 113/500, Loss: 0.0006190635779473398\n",
      "Epoch 114/500, Loss: 0.0004890054863478456\n",
      "Epoch 115/500, Loss: 0.0005208952373193045\n",
      "Epoch 116/500, Loss: 0.00042868310577302637\n",
      "Epoch 117/500, Loss: 0.00044696305745414326\n",
      "Epoch 118/500, Loss: 0.00032211283853809746\n",
      "Epoch 119/500, Loss: 0.0005095351495713528\n",
      "Epoch 120/500, Loss: 0.000433328519907913\n",
      "Epoch 121/500, Loss: 0.00043327335567612735\n",
      "Epoch 122/500, Loss: 0.00038415468173168065\n",
      "Epoch 123/500, Loss: 0.00044544630301451046\n",
      "Epoch 124/500, Loss: 0.0005653188792556259\n",
      "Epoch 125/500, Loss: 0.0003787261957768351\n",
      "Epoch 126/500, Loss: 0.0003720350463741592\n",
      "Epoch 127/500, Loss: 0.0004271329952254226\n",
      "Epoch 128/500, Loss: 0.00039416214817070534\n",
      "Epoch 129/500, Loss: 0.0003312502917002088\n",
      "Epoch 130/500, Loss: 0.0002891486302749919\n",
      "Epoch 131/500, Loss: 0.0003043068406571235\n",
      "Epoch 132/500, Loss: 0.0003737378792720847\n",
      "Epoch 133/500, Loss: 0.0003228592811085816\n",
      "Epoch 134/500, Loss: 0.0003266450990590134\n",
      "Epoch 135/500, Loss: 0.00023549895351087407\n",
      "Epoch 136/500, Loss: 0.00033033398046557395\n",
      "Epoch 137/500, Loss: 0.0003335542299152751\n",
      "Epoch 138/500, Loss: 0.00026933251397817263\n",
      "Epoch 139/500, Loss: 0.0002613914091073509\n",
      "Epoch 140/500, Loss: 0.0003968563521214362\n",
      "Epoch 141/500, Loss: 0.00022577982287787433\n",
      "Epoch 142/500, Loss: 0.00021320626443152184\n",
      "Epoch 143/500, Loss: 0.00031066532911998887\n",
      "Epoch 144/500, Loss: 0.00035435400994694125\n",
      "Epoch 145/500, Loss: 0.0002755625412516695\n",
      "Epoch 146/500, Loss: 0.00030338730305499794\n",
      "Epoch 147/500, Loss: 0.0002845675408025272\n",
      "Epoch 148/500, Loss: 0.00032194469531532377\n",
      "Epoch 149/500, Loss: 0.0002397138367606593\n",
      "Epoch 150/500, Loss: 0.0002870560565497726\n",
      "Epoch 151/500, Loss: 0.0002066235538222827\n",
      "Epoch 152/500, Loss: 0.00019486542026113187\n",
      "Epoch 153/500, Loss: 0.0003054910651241828\n",
      "Epoch 154/500, Loss: 0.00021646424803683267\n",
      "Epoch 155/500, Loss: 0.0002034335775533691\n",
      "Epoch 156/500, Loss: 0.00023355957611264394\n",
      "Epoch 157/500, Loss: 0.00023451450189375983\n",
      "Epoch 158/500, Loss: 0.00023864135749006112\n",
      "Epoch 159/500, Loss: 0.00018185738632122854\n",
      "Epoch 160/500, Loss: 0.0001711640361463651\n",
      "Epoch 161/500, Loss: 0.00021489824679779952\n",
      "Epoch 162/500, Loss: 0.00021801397815579548\n",
      "Epoch 163/500, Loss: 0.000217699830370423\n",
      "Epoch 164/500, Loss: 0.0001799831900695738\n",
      "Epoch 165/500, Loss: 0.0002275276690072912\n",
      "Epoch 166/500, Loss: 0.00022758800228725055\n",
      "Epoch 167/500, Loss: 0.0002734340045468083\n",
      "Epoch 168/500, Loss: 0.0001800233668680968\n",
      "Epoch 169/500, Loss: 0.0002016128944108329\n",
      "Epoch 170/500, Loss: 0.0002585073790604448\n",
      "Epoch 171/500, Loss: 0.00016751295957614535\n",
      "Epoch 172/500, Loss: 0.00013876091907150112\n",
      "Epoch 173/500, Loss: 0.00017545610691221164\n",
      "Epoch 174/500, Loss: 0.00016321610538787873\n",
      "Epoch 175/500, Loss: 0.0001783198832916761\n",
      "Epoch 176/500, Loss: 0.00022663478531675146\n",
      "Epoch 177/500, Loss: 0.00015741209946489626\n",
      "Epoch 178/500, Loss: 0.00016410767888633667\n",
      "Epoch 179/500, Loss: 0.0001544061243683765\n",
      "Epoch 180/500, Loss: 0.00018700628944705904\n",
      "Epoch 181/500, Loss: 0.00020698389042601257\n",
      "Epoch 182/500, Loss: 0.0002173306981733601\n",
      "Epoch 183/500, Loss: 0.00014013850367129116\n",
      "Epoch 184/500, Loss: 0.00014370941270109533\n",
      "Epoch 185/500, Loss: 0.00013062092018247182\n",
      "Epoch 186/500, Loss: 0.00015311046237392084\n",
      "Epoch 187/500, Loss: 0.00012638629206256674\n",
      "Epoch 188/500, Loss: 0.0001404777124532432\n",
      "Epoch 189/500, Loss: 0.00013581943286616088\n",
      "Epoch 190/500, Loss: 0.0001445439697168435\n",
      "Epoch 191/500, Loss: 0.0001328119927036044\n",
      "Epoch 192/500, Loss: 0.00014909196345667754\n",
      "Epoch 193/500, Loss: 0.00013955510491671572\n",
      "Epoch 194/500, Loss: 0.0001210716353463275\n",
      "Epoch 195/500, Loss: 0.00014943049297601516\n",
      "Epoch 196/500, Loss: 0.00011965459478752953\n",
      "Epoch 197/500, Loss: 0.00011561440104352576\n",
      "Epoch 198/500, Loss: 0.00012572141401636015\n",
      "Epoch 199/500, Loss: 0.00011318944598315284\n",
      "Epoch 200/500, Loss: 0.00013446808588923886\n",
      "Epoch 201/500, Loss: 0.00013720029736370116\n",
      "Epoch 202/500, Loss: 9.734482072027666e-05\n",
      "Epoch 203/500, Loss: 0.00014628977649928338\n",
      "Epoch 204/500, Loss: 0.00013475492648597407\n",
      "Epoch 205/500, Loss: 0.00017763226268081262\n",
      "Epoch 206/500, Loss: 0.00012900865242824824\n",
      "Epoch 207/500, Loss: 0.00015177220380532423\n",
      "Epoch 208/500, Loss: 0.00012339082708682066\n",
      "Epoch 209/500, Loss: 0.0001353509453890313\n",
      "Epoch 210/500, Loss: 0.00012102757474557231\n",
      "Epoch 211/500, Loss: 0.00011760226048603986\n",
      "Epoch 212/500, Loss: 0.00013885603766539134\n",
      "Epoch 213/500, Loss: 9.461301176218382e-05\n",
      "Epoch 214/500, Loss: 9.876236955668511e-05\n",
      "Epoch 215/500, Loss: 0.0001086589036276564\n",
      "Epoch 216/500, Loss: 0.00010865248103592811\n",
      "Epoch 217/500, Loss: 9.139951050331417e-05\n",
      "Epoch 218/500, Loss: 8.620066208615234e-05\n",
      "Epoch 219/500, Loss: 0.00010149421775297794\n",
      "Epoch 220/500, Loss: 8.140108288249135e-05\n",
      "Epoch 221/500, Loss: 0.0001032477812259458\n",
      "Epoch 222/500, Loss: 9.514059638604522e-05\n",
      "Epoch 223/500, Loss: 0.00012932385080992908\n",
      "Epoch 224/500, Loss: 0.00010189407943731307\n",
      "Epoch 225/500, Loss: 9.977992574152137e-05\n",
      "Epoch 226/500, Loss: 7.137349997979722e-05\n",
      "Epoch 227/500, Loss: 0.00010180666868109256\n",
      "Epoch 228/500, Loss: 0.00010295915334219379\n",
      "Epoch 229/500, Loss: 7.651938592191852e-05\n",
      "Epoch 230/500, Loss: 9.11648738630382e-05\n",
      "Epoch 231/500, Loss: 0.00011988591904810164\n",
      "Epoch 232/500, Loss: 5.409906797077773e-05\n",
      "Epoch 233/500, Loss: 8.218082634162524e-05\n",
      "Epoch 234/500, Loss: 0.00014259715369137536\n",
      "Epoch 235/500, Loss: 0.00010197741409813586\n",
      "Epoch 236/500, Loss: 6.526479254327049e-05\n",
      "Epoch 237/500, Loss: 9.429643016898938e-05\n",
      "Epoch 238/500, Loss: 8.095983503153548e-05\n",
      "Epoch 239/500, Loss: 7.632516904517875e-05\n",
      "Epoch 240/500, Loss: 0.00010094660891419542\n",
      "Epoch 241/500, Loss: 9.573708822634737e-05\n",
      "Epoch 242/500, Loss: 7.408261548594705e-05\n",
      "Epoch 243/500, Loss: 8.799608206442957e-05\n",
      "Epoch 244/500, Loss: 8.113107053629522e-05\n",
      "Epoch 245/500, Loss: 7.291858284068959e-05\n",
      "Epoch 246/500, Loss: 8.491378866892774e-05\n",
      "Epoch 247/500, Loss: 8.528849139111117e-05\n",
      "Epoch 248/500, Loss: 8.178148086049728e-05\n",
      "Epoch 249/500, Loss: 9.850992475029281e-05\n",
      "Epoch 250/500, Loss: 9.690241443292637e-05\n",
      "Epoch 251/500, Loss: 9.072936050610483e-05\n",
      "Epoch 252/500, Loss: 7.13281326950112e-05\n",
      "Epoch 253/500, Loss: 6.0137502649532894e-05\n",
      "Epoch 254/500, Loss: 6.212605252845346e-05\n",
      "Epoch 255/500, Loss: 6.577712026358183e-05\n",
      "Epoch 256/500, Loss: 7.226458806794003e-05\n",
      "Epoch 257/500, Loss: 6.25511698412343e-05\n",
      "Epoch 258/500, Loss: 5.7566057876101695e-05\n",
      "Epoch 259/500, Loss: 5.625021133580178e-05\n",
      "Epoch 260/500, Loss: 5.6992539612110704e-05\n",
      "Epoch 261/500, Loss: 8.27455821438759e-05\n",
      "Epoch 262/500, Loss: 9.538143993787733e-05\n",
      "Epoch 263/500, Loss: 5.614408915529826e-05\n",
      "Epoch 264/500, Loss: 7.556163992766025e-05\n",
      "Epoch 265/500, Loss: 5.439072940914359e-05\n",
      "Epoch 266/500, Loss: 6.623119874607905e-05\n",
      "Epoch 267/500, Loss: 6.167825735506735e-05\n",
      "Epoch 268/500, Loss: 9.750910196869102e-05\n",
      "Epoch 269/500, Loss: 5.964643553722583e-05\n",
      "Epoch 270/500, Loss: 6.290363411868125e-05\n",
      "Epoch 271/500, Loss: 4.678783651408074e-05\n",
      "Epoch 272/500, Loss: 6.350031227546944e-05\n",
      "Epoch 273/500, Loss: 6.021668559696991e-05\n",
      "Epoch 274/500, Loss: 5.5384199056009364e-05\n",
      "Epoch 275/500, Loss: 9.201379803666246e-05\n",
      "Epoch 276/500, Loss: 5.6449134717695415e-05\n",
      "Epoch 277/500, Loss: 6.821585598767601e-05\n",
      "Epoch 278/500, Loss: 7.498500638446006e-05\n",
      "Epoch 279/500, Loss: 7.948319294622966e-05\n",
      "Epoch 280/500, Loss: 4.6488323278026655e-05\n",
      "Epoch 281/500, Loss: 5.489362696867569e-05\n",
      "Epoch 282/500, Loss: 4.786419231095351e-05\n",
      "Epoch 283/500, Loss: 4.666279398536842e-05\n",
      "Epoch 284/500, Loss: 4.762558897449968e-05\n",
      "Epoch 285/500, Loss: 7.810419810994063e-05\n",
      "Epoch 286/500, Loss: 3.5896388128873825e-05\n",
      "Epoch 287/500, Loss: 5.802001864399894e-05\n",
      "Epoch 288/500, Loss: 5.757927517281912e-05\n",
      "Epoch 289/500, Loss: 6.371261380471489e-05\n",
      "Epoch 290/500, Loss: 5.6375584660729927e-05\n",
      "Epoch 291/500, Loss: 4.9746546470227516e-05\n",
      "Epoch 292/500, Loss: 5.3572044635075144e-05\n",
      "Epoch 293/500, Loss: 4.258503447220262e-05\n",
      "Epoch 294/500, Loss: 4.444483823005742e-05\n",
      "Epoch 295/500, Loss: 4.848063898472381e-05\n",
      "Epoch 296/500, Loss: 5.6344712772572946e-05\n",
      "Epoch 297/500, Loss: 5.593210802804346e-05\n",
      "Epoch 298/500, Loss: 4.60384749853152e-05\n",
      "Epoch 299/500, Loss: 4.86454334480056e-05\n",
      "Epoch 300/500, Loss: 4.607644612925859e-05\n",
      "Epoch 301/500, Loss: 7.108897001931577e-05\n",
      "Epoch 302/500, Loss: 3.3485191774421507e-05\n",
      "Epoch 303/500, Loss: 3.8566431092996413e-05\n",
      "Epoch 304/500, Loss: 5.7079915027965657e-05\n",
      "Epoch 305/500, Loss: 5.3976947518614384e-05\n",
      "Epoch 306/500, Loss: 3.717239321434006e-05\n",
      "Epoch 307/500, Loss: 4.9136776137207304e-05\n",
      "Epoch 308/500, Loss: 4.998765273610063e-05\n",
      "Epoch 309/500, Loss: 5.358419054703388e-05\n",
      "Epoch 310/500, Loss: 4.766782775992137e-05\n",
      "Epoch 311/500, Loss: 4.778627278158508e-05\n",
      "Epoch 312/500, Loss: 6.397962897608522e-05\n",
      "Epoch 313/500, Loss: 4.5473844173622115e-05\n",
      "Epoch 314/500, Loss: 3.194667078787461e-05\n",
      "Epoch 315/500, Loss: 4.897798472874066e-05\n",
      "Epoch 316/500, Loss: 3.943983704500299e-05\n",
      "Epoch 317/500, Loss: 4.8066465231905955e-05\n",
      "Epoch 318/500, Loss: 5.1901900535865154e-05\n",
      "Epoch 319/500, Loss: 4.5942258241536494e-05\n",
      "Epoch 320/500, Loss: 4.183270331330797e-05\n",
      "Epoch 321/500, Loss: 3.1178534819836e-05\n",
      "Epoch 322/500, Loss: 5.171556767891161e-05\n",
      "Epoch 323/500, Loss: 5.5728917297009115e-05\n",
      "Epoch 324/500, Loss: 3.831033843328312e-05\n",
      "Epoch 325/500, Loss: 4.1749072936779286e-05\n",
      "Epoch 326/500, Loss: 4.814088658479575e-05\n",
      "Epoch 327/500, Loss: 4.2416819529275275e-05\n",
      "Epoch 328/500, Loss: 5.044273569053205e-05\n",
      "Epoch 329/500, Loss: 3.248110819445823e-05\n",
      "Epoch 330/500, Loss: 3.443464980331815e-05\n",
      "Epoch 331/500, Loss: 3.130272853013594e-05\n",
      "Epoch 332/500, Loss: 2.8896181122815635e-05\n",
      "Epoch 333/500, Loss: 3.2668015072496405e-05\n",
      "Epoch 334/500, Loss: 2.8455103737152448e-05\n",
      "Epoch 335/500, Loss: 3.2056187852244224e-05\n",
      "Epoch 336/500, Loss: 3.144570538487252e-05\n",
      "Epoch 337/500, Loss: 3.4839949291511273e-05\n",
      "Epoch 338/500, Loss: 3.741392694272301e-05\n",
      "Epoch 339/500, Loss: 3.265587396786681e-05\n",
      "Epoch 340/500, Loss: 4.5755895111402166e-05\n",
      "Epoch 341/500, Loss: 3.12459009624685e-05\n",
      "Epoch 342/500, Loss: 2.660343138164275e-05\n",
      "Epoch 343/500, Loss: 3.835009556496516e-05\n",
      "Epoch 344/500, Loss: 3.2128910658814546e-05\n",
      "Epoch 345/500, Loss: 4.179625700219601e-05\n",
      "Epoch 346/500, Loss: 5.52508460844235e-05\n",
      "Epoch 347/500, Loss: 3.245020837182112e-05\n",
      "Epoch 348/500, Loss: 3.1571405997965485e-05\n",
      "Epoch 349/500, Loss: 2.617768034594649e-05\n",
      "Epoch 350/500, Loss: 3.513141629290268e-05\n",
      "Epoch 351/500, Loss: 2.1843265585630434e-05\n",
      "Epoch 352/500, Loss: 2.508950092305895e-05\n",
      "Epoch 353/500, Loss: 2.8810038202209398e-05\n",
      "Epoch 354/500, Loss: 2.750086144130494e-05\n",
      "Epoch 355/500, Loss: 3.335415775446953e-05\n",
      "Epoch 356/500, Loss: 3.110559737251606e-05\n",
      "Epoch 357/500, Loss: 3.1483553617103356e-05\n",
      "Epoch 358/500, Loss: 2.543297718407952e-05\n",
      "Epoch 359/500, Loss: 2.5384599341902814e-05\n",
      "Epoch 360/500, Loss: 2.127096104231896e-05\n",
      "Epoch 361/500, Loss: 2.0922849736442523e-05\n",
      "Epoch 362/500, Loss: 2.8501270077997886e-05\n",
      "Epoch 363/500, Loss: 2.3210930131816504e-05\n",
      "Epoch 364/500, Loss: 3.1051131372805685e-05\n",
      "Epoch 365/500, Loss: 3.460293282842031e-05\n",
      "Epoch 366/500, Loss: 3.591361408845322e-05\n",
      "Epoch 367/500, Loss: 3.0939903029190775e-05\n",
      "Epoch 368/500, Loss: 2.7329008324678788e-05\n",
      "Epoch 369/500, Loss: 3.336813129603148e-05\n",
      "Epoch 370/500, Loss: 3.806064705713652e-05\n",
      "Epoch 371/500, Loss: 3.276219532771003e-05\n",
      "Epoch 372/500, Loss: 2.9554048394077525e-05\n",
      "Epoch 373/500, Loss: 3.391984669828422e-05\n",
      "Epoch 374/500, Loss: 3.2079508725603644e-05\n",
      "Epoch 375/500, Loss: 4.407885249487923e-05\n",
      "Epoch 376/500, Loss: 2.8132262091925702e-05\n",
      "Epoch 377/500, Loss: 2.5556343514056478e-05\n",
      "Epoch 378/500, Loss: 2.862960679652003e-05\n",
      "Epoch 379/500, Loss: 2.6282781878502908e-05\n",
      "Epoch 380/500, Loss: 3.200162453660076e-05\n",
      "Epoch 381/500, Loss: 2.6193813937425148e-05\n",
      "Epoch 382/500, Loss: 3.3122172031393605e-05\n",
      "Epoch 383/500, Loss: 1.925237354173857e-05\n",
      "Epoch 384/500, Loss: 2.514346215320984e-05\n",
      "Epoch 385/500, Loss: 1.692431682645942e-05\n",
      "Epoch 386/500, Loss: 2.751514044315887e-05\n",
      "Epoch 387/500, Loss: 2.1585806020344273e-05\n",
      "Epoch 388/500, Loss: 2.7451564684659907e-05\n",
      "Epoch 389/500, Loss: 2.127720910851037e-05\n",
      "Epoch 390/500, Loss: 3.0224846601153592e-05\n",
      "Epoch 391/500, Loss: 3.9396286670775483e-05\n",
      "Epoch 392/500, Loss: 3.294835551059805e-05\n",
      "Epoch 393/500, Loss: 3.1293942551461183e-05\n",
      "Epoch 394/500, Loss: 3.055411213738678e-05\n",
      "Epoch 395/500, Loss: 2.2249236280913465e-05\n",
      "Epoch 396/500, Loss: 2.795313217315457e-05\n",
      "Epoch 397/500, Loss: 3.618632529521294e-05\n",
      "Epoch 398/500, Loss: 2.515990685684041e-05\n",
      "Epoch 399/500, Loss: 2.0116561959834405e-05\n",
      "Epoch 400/500, Loss: 2.9385527990858202e-05\n",
      "Epoch 401/500, Loss: 1.7372810491776492e-05\n",
      "Epoch 402/500, Loss: 1.7041129272651494e-05\n",
      "Epoch 403/500, Loss: 2.0861712593094645e-05\n",
      "Epoch 404/500, Loss: 2.3906189848535826e-05\n",
      "Epoch 405/500, Loss: 2.1593415112874936e-05\n",
      "Epoch 406/500, Loss: 1.5562861855349703e-05\n",
      "Epoch 407/500, Loss: 4.398512204975954e-05\n",
      "Epoch 408/500, Loss: 2.383038021694769e-05\n",
      "Epoch 409/500, Loss: 3.8248603751916175e-05\n",
      "Epoch 410/500, Loss: 2.0407490150578917e-05\n",
      "Epoch 411/500, Loss: 1.7589778248553296e-05\n",
      "Epoch 412/500, Loss: 1.6795181441661305e-05\n",
      "Epoch 413/500, Loss: 3.064152341331854e-05\n",
      "Epoch 414/500, Loss: 1.7849595094178638e-05\n",
      "Epoch 415/500, Loss: 2.318559849559928e-05\n",
      "Epoch 416/500, Loss: 1.753418226663988e-05\n",
      "Epoch 417/500, Loss: 2.2165590378010946e-05\n",
      "Epoch 418/500, Loss: 1.8887234676055543e-05\n",
      "Epoch 419/500, Loss: 2.4106691073809217e-05\n",
      "Epoch 420/500, Loss: 2.7115415183029006e-05\n",
      "Epoch 421/500, Loss: 2.148830240652647e-05\n",
      "Epoch 422/500, Loss: 2.632885077348744e-05\n",
      "Epoch 423/500, Loss: 1.831941582557712e-05\n",
      "Epoch 424/500, Loss: 3.0373953580107938e-05\n",
      "Epoch 425/500, Loss: 1.8466819840666305e-05\n",
      "Epoch 426/500, Loss: 1.6955471957252094e-05\n",
      "Epoch 427/500, Loss: 2.027240329230803e-05\n",
      "Epoch 428/500, Loss: 2.1057159886238098e-05\n",
      "Epoch 429/500, Loss: 2.3194445930130314e-05\n",
      "Epoch 430/500, Loss: 1.7564153625114288e-05\n",
      "Epoch 431/500, Loss: 1.764173131440267e-05\n",
      "Epoch 432/500, Loss: 2.1401343149461483e-05\n",
      "Epoch 433/500, Loss: 1.4954305860166122e-05\n",
      "Epoch 434/500, Loss: 1.3598668569362157e-05\n",
      "Epoch 435/500, Loss: 2.025251082419085e-05\n",
      "Epoch 436/500, Loss: 1.813707324669979e-05\n",
      "Epoch 437/500, Loss: 1.7641176912418033e-05\n",
      "Epoch 438/500, Loss: 1.8660500440351564e-05\n",
      "Epoch 439/500, Loss: 2.092215657155196e-05\n",
      "Epoch 440/500, Loss: 1.8510702830099035e-05\n",
      "Epoch 441/500, Loss: 1.9734468553776552e-05\n",
      "Epoch 442/500, Loss: 1.4710789790634798e-05\n",
      "Epoch 443/500, Loss: 1.387817337672459e-05\n",
      "Epoch 444/500, Loss: 1.5132327462197281e-05\n",
      "Epoch 445/500, Loss: 2.373357762865323e-05\n",
      "Epoch 446/500, Loss: 1.853032556807323e-05\n",
      "Epoch 447/500, Loss: 2.0374396366865506e-05\n",
      "Epoch 448/500, Loss: 1.724144054346003e-05\n",
      "Epoch 449/500, Loss: 1.866360253188759e-05\n",
      "Epoch 450/500, Loss: 1.1435239944798273e-05\n",
      "Epoch 451/500, Loss: 1.9537472975831145e-05\n",
      "Epoch 452/500, Loss: 1.2175203015171324e-05\n",
      "Epoch 453/500, Loss: 1.731587352488922e-05\n",
      "Epoch 454/500, Loss: 1.2637644041595714e-05\n",
      "Epoch 455/500, Loss: 1.8986014181012122e-05\n",
      "Epoch 456/500, Loss: 1.5825127255603938e-05\n",
      "Epoch 457/500, Loss: 1.5195974745958146e-05\n",
      "Epoch 458/500, Loss: 1.6296468207396435e-05\n",
      "Epoch 459/500, Loss: 2.364847516998582e-05\n",
      "Epoch 460/500, Loss: 1.4046090200281469e-05\n",
      "Epoch 461/500, Loss: 1.6004604601351146e-05\n",
      "Epoch 462/500, Loss: 1.707998723889302e-05\n",
      "Epoch 463/500, Loss: 1.5643036022083834e-05\n",
      "Epoch 464/500, Loss: 1.5390793773154394e-05\n",
      "Epoch 465/500, Loss: 1.0588233765572244e-05\n",
      "Epoch 466/500, Loss: 1.457562341654141e-05\n",
      "Epoch 467/500, Loss: 2.7084053564457073e-05\n",
      "Epoch 468/500, Loss: 1.4000195018784975e-05\n",
      "Epoch 469/500, Loss: 1.4586254759966063e-05\n",
      "Epoch 470/500, Loss: 1.448262303581162e-05\n",
      "Epoch 471/500, Loss: 1.446350490498714e-05\n",
      "Epoch 472/500, Loss: 1.5836041581808657e-05\n",
      "Epoch 473/500, Loss: 1.1712439507911248e-05\n",
      "Epoch 474/500, Loss: 1.2016008960407426e-05\n",
      "Epoch 475/500, Loss: 1.0000649841198797e-05\n",
      "Epoch 476/500, Loss: 1.0172517834559716e-05\n",
      "Epoch 477/500, Loss: 1.9508438526827376e-05\n",
      "Epoch 478/500, Loss: 1.3772296305043191e-05\n",
      "Epoch 479/500, Loss: 1.2410236779812425e-05\n",
      "Epoch 480/500, Loss: 1.1003255362343875e-05\n",
      "Epoch 481/500, Loss: 1.5742801419297132e-05\n",
      "Epoch 482/500, Loss: 1.2867010225038808e-05\n",
      "Epoch 483/500, Loss: 1.0734994053304295e-05\n",
      "Epoch 484/500, Loss: 1.1383302860070086e-05\n",
      "Epoch 485/500, Loss: 1.2307279965690604e-05\n",
      "Epoch 486/500, Loss: 1.1455024865166966e-05\n",
      "Epoch 487/500, Loss: 1.408076994786305e-05\n",
      "Epoch 488/500, Loss: 1.2690788902805902e-05\n",
      "Epoch 489/500, Loss: 1.3901990119588195e-05\n",
      "Epoch 490/500, Loss: 1.2243455785210244e-05\n",
      "Epoch 491/500, Loss: 1.4721642189994912e-05\n",
      "Epoch 492/500, Loss: 1.2622721572240283e-05\n",
      "Epoch 493/500, Loss: 1.3747510942201708e-05\n",
      "Epoch 494/500, Loss: 1.3294050599402649e-05\n",
      "Epoch 495/500, Loss: 1.0930473958329198e-05\n",
      "Epoch 496/500, Loss: 1.8975760570226286e-05\n",
      "Epoch 497/500, Loss: 1.0493312272176678e-05\n",
      "Epoch 498/500, Loss: 1.2003369517645166e-05\n",
      "Epoch 499/500, Loss: 1.4652850625550076e-05\n",
      "Epoch 500/500, Loss: 1.6534249490567682e-05\n",
      "訓練完成！\n"
     ]
    }
   ],
   "source": [
    "# 設置設備（如果有 GPU 就用 GPU）\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 建立模型、損失函數和優化器\n",
    "model = SimpleCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "# 訓練模型\n",
    "num_epochs = 500\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # 初始化梯度\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 前向傳播\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # 反向傳播與優化\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 自動生成 24 個月份的列表，從 2021 年 1 月開始"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_date_list(start_date, months):\n",
    "    date_list = []\n",
    "    start = datetime.strptime(start_date, \"%Y%m%d\")\n",
    "    for i in range(months):\n",
    "        year_month = start + timedelta(days=30 * i)  # 每次增加一個月\n",
    "        date_str = year_month.strftime(\"%Y%m01\")  # 每個月的第一天\n",
    "        date_list.append(date_str)\n",
    "    return date_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import mplfinance as mpf\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將 Matplotlib 圖像 fig 轉換為 PIL Image\n",
    "def fig_to_image(fig):\n",
    "    # 將圖像渲染為二進位 RGB 數據\n",
    "    fig.canvas.draw()\n",
    "    img = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
    "    img = img.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "    \n",
    "    # 將 numpy array 轉換為 PIL Image\n",
    "    pil_image = Image.fromarray(img)\n",
    "    return pil_image\n",
    "\n",
    "# 將 PIL Image 轉換為 PyTorch 張量\n",
    "def image_to_tensor(pil_image):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((64, 64)),  # 確保圖像大小與模型輸入匹配\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    tensor_image = transform(pil_image)\n",
    "    return tensor_image.unsqueeze(0)  # 增加批次維度 [1, C, H, W]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 股票代碼\n",
    "stock_id = \"2330.TW\"\n",
    "\n",
    "# 過去十年資料，加上四天來計算五日移動平均\n",
    "start_date = \"2024-01-01\"  # 提前四天\n",
    "end_date = \"2024-09-01\"\n",
    "interval_days = 4  # 每5天截圖一次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-01-08~2024-01-12 Prediction:漲\n",
      "2024-01-08~2024-01-12 Truth：跌\n",
      "2024-03-04~2024-03-08 Prediction:漲\n",
      "2024-03-04~2024-03-08 Truth：漲\n",
      "2024-04-15~2024-04-19 Prediction:跌\n",
      "2024-04-15~2024-04-19 Truth：跌\n",
      "2024-05-20~2024-05-24 Prediction:漲\n",
      "2024-05-20~2024-05-24 Truth：漲\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\謝孟達\\AppData\\Local\\Temp\\ipykernel_26568\\4196208861.py:5: MatplotlibDeprecationWarning: The tostring_rgb function was deprecated in Matplotlib 3.8 and will be removed two minor releases later. Use buffer_rgba instead.\n",
      "  img = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
      "C:\\Users\\謝孟達\\AppData\\Local\\Temp\\ipykernel_26568\\4196208861.py:5: MatplotlibDeprecationWarning: The tostring_rgb function was deprecated in Matplotlib 3.8 and will be removed two minor releases later. Use buffer_rgba instead.\n",
      "  img = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
      "C:\\Users\\謝孟達\\AppData\\Local\\Temp\\ipykernel_26568\\4196208861.py:5: MatplotlibDeprecationWarning: The tostring_rgb function was deprecated in Matplotlib 3.8 and will be removed two minor releases later. Use buffer_rgba instead.\n",
      "  img = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
      "C:\\Users\\謝孟達\\AppData\\Local\\Temp\\ipykernel_26568\\4196208861.py:5: MatplotlibDeprecationWarning: The tostring_rgb function was deprecated in Matplotlib 3.8 and will be removed two minor releases later. Use buffer_rgba instead.\n",
      "  img = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n"
     ]
    }
   ],
   "source": [
    "# 下載股票資料並移除缺失值\n",
    "ticker = yf.Ticker(stock_id)\n",
    "stock_data = ticker.history(start=start_date, end=end_date, interval=\"1d\")\n",
    "stock_data = stock_data.apply(pd.to_numeric, errors='coerce').dropna().astype(float)\n",
    "\n",
    "# 計算五日均線\n",
    "stock_data['SMA_5'] = stock_data['Close'].rolling(window=5).mean()\n",
    "\n",
    "\n",
    "for i in range(0, len(stock_data), interval_days):\n",
    "    idx = stock_data.index[i]\n",
    "    plot_data = stock_data.loc[idx - pd.Timedelta(days=4):idx]  # 繪製該日期前的完整數據\n",
    "\n",
    "    # 檢查是否有足夠的數據\n",
    "    if len(plot_data) < 5:  # 確保5天數據充足\n",
    "        continue\n",
    "\n",
    "    # 繪製 K 線圖，不顯示 SMA\n",
    "    fig, ax = mpf.plot(\n",
    "        plot_data,\n",
    "        type='candle',\n",
    "        style='charles',\n",
    "        ylabel=\"\",\n",
    "        volume=False,\n",
    "        xrotation=0,\n",
    "        returnfig=True\n",
    "    )\n",
    "    # 移除 X, Y 軸的刻度與標籤\n",
    "    ax[0].set_xticks([])\n",
    "    ax[0].set_yticks([])\n",
    "    ax[0].set_xlabel(\"\")\n",
    "    ax[0].set_ylabel(\"\")\n",
    "    ax[0].set_title(\"\")\n",
    "\n",
    "    pil_image = fig_to_image(fig)\n",
    "    tensor_image = image_to_tensor(pil_image).to(device)\n",
    "    plt.close(fig) \n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(tensor_image)\n",
    "        prediction = torch.argmax(output, dim=1).item()\n",
    "\n",
    "    row = plot_data.iloc[-1]  # 取當前最後一天的數據\n",
    "    trend_label = 1 if row['Close'] > row['SMA_5'] else 0\n",
    "    print(f\"{(idx - pd.Timedelta(days=4)).strftime('%Y-%m-%d')}~{idx.strftime('%Y-%m-%d')} Prediction:{'漲' if prediction==1 else '跌'}\")\n",
    "    print(f\"{(idx - pd.Timedelta(days=4)).strftime('%Y-%m-%d')}~{idx.strftime('%Y-%m-%d')} Truth：{'漲' if trend_label==1 else '跌'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
